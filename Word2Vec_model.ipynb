{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the  dataset\n",
    "with open (\"C:/Users/Aakash Sadaphule/Desktop/Untitled Folder/archive/MMHS150K_GT.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame.from_dict(data)\n",
    "data=data.T\n",
    "data=data.reset_index()\n",
    "data.rename(columns={\"index\":\"user_id\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows: 149823 and columns are: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>img_url</th>\n",
       "      <th>labels</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>labels_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1114679353714016256</td>\n",
       "      <td>http://pbs.twimg.com/tweet_video_thumb/D3gi9MH...</td>\n",
       "      <td>[4, 1, 3]</td>\n",
       "      <td>https://twitter.com/user/status/11146793537140...</td>\n",
       "      <td>@FriskDontMiss Nigga https://t.co/cAsaLWEpue</td>\n",
       "      <td>[Religion, Racist, Homophobe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1063020048816660480</td>\n",
       "      <td>http://pbs.twimg.com/ext_tw_video_thumb/106301...</td>\n",
       "      <td>[5, 5, 5]</td>\n",
       "      <td>https://twitter.com/user/status/10630200488166...</td>\n",
       "      <td>My horses are retarded https://t.co/HYhqc6d5WN</td>\n",
       "      <td>[OtherHate, OtherHate, OtherHate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1108927368075374593</td>\n",
       "      <td>http://pbs.twimg.com/media/D2OzhzHUwAADQjd.jpg</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://twitter.com/user/status/11089273680753...</td>\n",
       "      <td>“NIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL S...</td>\n",
       "      <td>[NotHate, NotHate, NotHate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1114558534635618305</td>\n",
       "      <td>http://pbs.twimg.com/ext_tw_video_thumb/111401...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>https://twitter.com/user/status/11145585346356...</td>\n",
       "      <td>RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...</td>\n",
       "      <td>[Racist, NotHate, NotHate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1035252480215592966</td>\n",
       "      <td>http://pbs.twimg.com/media/Dl30pGIU8AAVGxO.jpg</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "      <td>https://twitter.com/user/status/10352524802155...</td>\n",
       "      <td>“EVERYbody calling you Nigger now!” https://t....</td>\n",
       "      <td>[Racist, NotHate, Racist]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id                                            img_url  \\\n",
       "0  1114679353714016256  http://pbs.twimg.com/tweet_video_thumb/D3gi9MH...   \n",
       "1  1063020048816660480  http://pbs.twimg.com/ext_tw_video_thumb/106301...   \n",
       "2  1108927368075374593     http://pbs.twimg.com/media/D2OzhzHUwAADQjd.jpg   \n",
       "3  1114558534635618305  http://pbs.twimg.com/ext_tw_video_thumb/111401...   \n",
       "4  1035252480215592966     http://pbs.twimg.com/media/Dl30pGIU8AAVGxO.jpg   \n",
       "\n",
       "      labels                                          tweet_url  \\\n",
       "0  [4, 1, 3]  https://twitter.com/user/status/11146793537140...   \n",
       "1  [5, 5, 5]  https://twitter.com/user/status/10630200488166...   \n",
       "2  [0, 0, 0]  https://twitter.com/user/status/11089273680753...   \n",
       "3  [1, 0, 0]  https://twitter.com/user/status/11145585346356...   \n",
       "4  [1, 0, 1]  https://twitter.com/user/status/10352524802155...   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "0       @FriskDontMiss Nigga https://t.co/cAsaLWEpue   \n",
       "1     My horses are retarded https://t.co/HYhqc6d5WN   \n",
       "2  “NIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL S...   \n",
       "3  RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...   \n",
       "4  “EVERYbody calling you Nigger now!” https://t....   \n",
       "\n",
       "                          labels_str  \n",
       "0      [Religion, Racist, Homophobe]  \n",
       "1  [OtherHate, OtherHate, OtherHate]  \n",
       "2        [NotHate, NotHate, NotHate]  \n",
       "3         [Racist, NotHate, NotHate]  \n",
       "4          [Racist, NotHate, Racist]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('No. of rows: {} and columns are: {}'.format(data.shape[0],data.shape[1]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aakash\n",
      "[nltk_data]     Sadaphule\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to C:\\Users\\Aakash\n",
      "[nltk_data]     Sadaphule\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Aakash\n",
      "[nltk_data]     Sadaphule\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing packages for data preporcessing\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')\n",
    "nltk.download('punkt')\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preporcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.remove('not')\n",
    "    text_data = []\n",
    "    for tweet in data:\n",
    "      review = re.sub(r\"@[A-Za-z0-9_]+\", \" \", tweet)\n",
    "      review = re.sub('RT', ' ', review)\n",
    "      review = re.sub(r\"https?://[A-Za-z0-9./]+\", \" \", review)\n",
    "      review = re.sub(r\"https?\", \" \", review)\n",
    "      review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "      review = review.lower()\n",
    "      review = review.split()\n",
    "      porter_stemmer = PorterStemmer()\n",
    "      review = [porter_stemmer.stem(word) for word in review if not word in set(stop_words) if len(word) > 2]\n",
    "      review = ' '.join(review)\n",
    "      text_data.append(review)\n",
    "\n",
    "    return np.array(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nigga', 'hors retard',\n",
       "       'nigga momma youngboy spit real shit nigga', ...,\n",
       "       'nigga big shitti',\n",
       "       'say nigga rich amp said anger mmph lhhni lhhnyreunion',\n",
       "       'nigga joe budden said thano got galact ass'], dtype='<U110')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = preprocessing(data['tweet_text'])\n",
    "text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "dataset_directory=\"C:/Users/Aakash Sadaphule/Desktop/Untitled Folder/archive\"\n",
    "with open(join(dataset_directory,'MMHS150K_GT.json')) as json_file:\n",
    "    HS_data = json.load(json_file)\n",
    "hate_data = HS_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Religion'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "def get_max_item(a_dict):\n",
    "  return max(a_dict.items(), key=operator.itemgetter(1))[0]\n",
    "get_max_item({'Religion': 1, 'Racist': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_single(labels):\n",
    "  label_dict={}\n",
    "  for l in labels:\n",
    "    if l not in label_dict:\n",
    "      label_dict[l] = 1\n",
    "    else:\n",
    "      label_dict[l] += 1\n",
    "  return get_max_item(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_data = [[key, val['tweet_text'], val['labels_str'], labels_to_single(val['labels_str'])] \n",
    "              # 1 if sum(to_label(val['labels'])) == 3 else 0] \n",
    "             for key, val in hate_data.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = [row[1] for row in hs_data]\n",
    "y = [row[3] for row in hs_data]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = \"?:!.,;[]-\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def tokenize_and_remove_stop_words(text):\n",
    "    #word_tokenizer = NLTKWordTokenizer()\n",
    "    # inputs = ['']\n",
    "\n",
    "    raw_tokens = word_tokenize(text)\n",
    "    final_tokens = [token.lower() for token in raw_tokens\n",
    "            if token not in punctuations and token not in stop_words]\n",
    "    # print(final_tokens)\n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "def lemmatize(words):\n",
    "    return [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "\n",
    "def to_word2vec_vector(sent_items):\n",
    "    model = Word2Vec(sent_items, min_count=1, vector_size=50, workers=3, window=3, sg=1)\n",
    "    return model\n",
    "\n",
    "def get_sentence_vectors(word2vec_model, sentence_tokens):\n",
    "    res = []\n",
    "    for sentence in sentence_tokens:  # TODO: fix two loops\n",
    "        sentence_vectors = np.array([word2vec_model.wv[token] for token in sentence])\n",
    "        mean = np.mean(sentence_vectors, axis=0)\n",
    "        res.append(mean)\n",
    "\n",
    "    return np.array(res)\n",
    "\n",
    "def vectorize_by_wor2vec(text_instances):\n",
    "  tokenized_items = []\n",
    "  for ti in text_instances:\n",
    "      tokens = tokenize_and_remove_stop_words(ti)\n",
    "      tokens = lemmatize(tokens)\n",
    "      # print(tokens)\n",
    "      tokenized_items.append(tokens)\n",
    "\n",
    "  #print(tokenized_items)\n",
    "  model = to_word2vec_vector(tokenized_items)\n",
    "  print(f'shape of word2vec model (vocabulary size) is: {model.cum_table.shape}')\n",
    "  #print(model.cum_table)\n",
    "  sen_vectors = get_sentence_vectors(model, tokenized_items)\n",
    "  return sen_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of word2vec model (vocabulary size) is: (228740,)\n",
      "shape of word2vec model (vocabulary size) is: (68795,)\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorize_by_wor2vec(X_train)\n",
    "X_val  = vectorize_by_wor2vec(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(10,), random_state=1,\n",
       "              solver='sgd')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_clf = MLPClassifier(solver='sgd',\n",
    "                    alpha=1e-5,\n",
    "                    hidden_layer_sizes=(10,),\n",
    "                    random_state=1)\n",
    "mlp_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = mlp_clf.predict(X_train)\n",
    "val_pred = mlp_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report metrics like precision, recall, F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of training set is :  0.781115987251581\n",
      "Accuracy of validation set is:  0.7763056899716336\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy of training set is : \", accuracy_score(y_train, train_pred))\n",
    "\n",
    "print(\"Accuracy of validation set is: \", accuracy_score(y_val, val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision value on training data is :0.719\n",
      "\n",
      "Precision value on validation data is :0.625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_train,train_pred, pos_label= 1, average= 'weighted')\n",
    "precision1 = precision_score(y_val,val_pred, pos_label= 1, average= 'weighted')\n",
    "print('Precision value on training data is :%.3f\\n' % precision)\n",
    "print('Precision value on validation data is :%.3f\\n' % precision1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall value on training data is : 0.781\n",
      "\n",
      "Recall value on validation data is : 0.776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_train,train_pred,average= 'micro')\n",
    "recall1 = recall_score(y_val,val_pred,average= 'micro')\n",
    "print('Recall value on training data is : %.3f\\n' % recall)\n",
    "print('Recall value on validation data is : %.3f\\n' % recall1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of training data is : 0.714\n",
      "\n",
      "F1 score of validation data is : 0.685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_train,train_pred, pos_label= '1',average= 'weighted')\n",
    "f1_val = f1_score(y_val,val_pred, pos_label= '1',average= 'weighted')\n",
    "print('F1 score of training data is : %.3f\\n' % f1)\n",
    "print('F1 score of validation data is : %.3f\\n' % f1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
