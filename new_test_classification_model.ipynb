{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Aakash Sadaphule\\\\Desktop\\\\Untitled Folder'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the dataset\n",
    "dataset_path = 'C:/Users/Aakash Sadaphule/Desktop/Untitled Folder/archive/MMHS150K_GT.json'\n",
    "with open(dataset_path, 'r') as f:\n",
    "  hs_data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"C:/Users/Aakash Sadaphule/Desktop/Untitled Folder/archive/MMHS150K_GT.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame.from_dict(data)\n",
    "data=data.T\n",
    "data=data.reset_index()\n",
    "data.rename(columns={\"index\":\"user_id\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows: 149823 and columns are: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>img_url</th>\n",
       "      <th>labels</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>labels_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1114679353714016256</td>\n",
       "      <td>http://pbs.twimg.com/tweet_video_thumb/D3gi9MH...</td>\n",
       "      <td>[4, 1, 3]</td>\n",
       "      <td>https://twitter.com/user/status/11146793537140...</td>\n",
       "      <td>@FriskDontMiss Nigga https://t.co/cAsaLWEpue</td>\n",
       "      <td>[Religion, Racist, Homophobe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1063020048816660480</td>\n",
       "      <td>http://pbs.twimg.com/ext_tw_video_thumb/106301...</td>\n",
       "      <td>[5, 5, 5]</td>\n",
       "      <td>https://twitter.com/user/status/10630200488166...</td>\n",
       "      <td>My horses are retarded https://t.co/HYhqc6d5WN</td>\n",
       "      <td>[OtherHate, OtherHate, OtherHate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1108927368075374593</td>\n",
       "      <td>http://pbs.twimg.com/media/D2OzhzHUwAADQjd.jpg</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://twitter.com/user/status/11089273680753...</td>\n",
       "      <td>“NIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL S...</td>\n",
       "      <td>[NotHate, NotHate, NotHate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1114558534635618305</td>\n",
       "      <td>http://pbs.twimg.com/ext_tw_video_thumb/111401...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>https://twitter.com/user/status/11145585346356...</td>\n",
       "      <td>RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...</td>\n",
       "      <td>[Racist, NotHate, NotHate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1035252480215592966</td>\n",
       "      <td>http://pbs.twimg.com/media/Dl30pGIU8AAVGxO.jpg</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "      <td>https://twitter.com/user/status/10352524802155...</td>\n",
       "      <td>“EVERYbody calling you Nigger now!” https://t....</td>\n",
       "      <td>[Racist, NotHate, Racist]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id                                            img_url  \\\n",
       "0  1114679353714016256  http://pbs.twimg.com/tweet_video_thumb/D3gi9MH...   \n",
       "1  1063020048816660480  http://pbs.twimg.com/ext_tw_video_thumb/106301...   \n",
       "2  1108927368075374593     http://pbs.twimg.com/media/D2OzhzHUwAADQjd.jpg   \n",
       "3  1114558534635618305  http://pbs.twimg.com/ext_tw_video_thumb/111401...   \n",
       "4  1035252480215592966     http://pbs.twimg.com/media/Dl30pGIU8AAVGxO.jpg   \n",
       "\n",
       "      labels                                          tweet_url  \\\n",
       "0  [4, 1, 3]  https://twitter.com/user/status/11146793537140...   \n",
       "1  [5, 5, 5]  https://twitter.com/user/status/10630200488166...   \n",
       "2  [0, 0, 0]  https://twitter.com/user/status/11089273680753...   \n",
       "3  [1, 0, 0]  https://twitter.com/user/status/11145585346356...   \n",
       "4  [1, 0, 1]  https://twitter.com/user/status/10352524802155...   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "0       @FriskDontMiss Nigga https://t.co/cAsaLWEpue   \n",
       "1     My horses are retarded https://t.co/HYhqc6d5WN   \n",
       "2  “NIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL S...   \n",
       "3  RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...   \n",
       "4  “EVERYbody calling you Nigger now!” https://t....   \n",
       "\n",
       "                          labels_str  \n",
       "0      [Religion, Racist, Homophobe]  \n",
       "1  [OtherHate, OtherHate, OtherHate]  \n",
       "2        [NotHate, NotHate, NotHate]  \n",
       "3         [Racist, NotHate, NotHate]  \n",
       "4          [Racist, NotHate, Racist]  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('No. of rows: {} and columns are: {}'.format(data.shape[0],data.shape[1]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aakash\n",
      "[nltk_data]     Sadaphule\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to C:\\Users\\Aakash\n",
      "[nltk_data]     Sadaphule\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Aakash\n",
      "[nltk_data]     Sadaphule\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')\n",
    "nltk.download('punkt')\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preporcessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.remove('not')\n",
    "    text_data = []\n",
    "    for tweet in data:\n",
    "      review = re.sub(r\"@[A-Za-z0-9_]+\", \" \", tweet)\n",
    "      review = re.sub('RT', ' ', review)\n",
    "      review = re.sub(r\"https?://[A-Za-z0-9./]+\", \" \", review)\n",
    "      review = re.sub(r\"https?\", \" \", review)\n",
    "      review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "      review = review.lower()\n",
    "      review = review.split()\n",
    "      porter_stemmer = PorterStemmer()\n",
    "      review = [porter_stemmer.stem(word) for word in review if not word in set(stop_words) if len(word) > 2]\n",
    "      review = ' '.join(review)\n",
    "      text_data.append(review)\n",
    "\n",
    "    return np.array(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nigga', 'hors retard',\n",
       "       'nigga momma youngboy spit real shit nigga', ...,\n",
       "       'nigga big shitti',\n",
       "       'say nigga rich amp said anger mmph lhhni lhhnyreunion',\n",
       "       'nigga joe budden said thano got galact ass'], dtype='<U110')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = preprocessing(data['tweet_text'])\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         4\n",
       "1         5\n",
       "2         0\n",
       "3         1\n",
       "4         1\n",
       "         ..\n",
       "149818    2\n",
       "149819    0\n",
       "149820    0\n",
       "149821    1\n",
       "149822    0\n",
       "Name: new_label, Length: 149823, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nd = data.copy()\n",
    "Nd['labels'].value_counts\n",
    "new_label = []\n",
    "for x in Nd['labels'].values:\n",
    "    new_label.append(x[0])\n",
    "    \n",
    "Nd['new_label'] = new_label\n",
    "Nd['new_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1114679353714016256', '@FriskDontMiss Nigga https://t.co/cAsaLWEpue', 1],\n",
       " ['1063020048816660480', 'My horses are retarded https://t.co/HYhqc6d5WN', 1],\n",
       " ['1108927368075374593',\n",
       "  '“NIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL SHIT NIGGA” https://t.co/UczofqHrLq',\n",
       "  0],\n",
       " ['1114558534635618305',\n",
       "  'RT xxSuGVNGxx: I ran into this HOLY NIGGA TODAY 😭😭😭😭 https://t.co/Wa6Spl9kIw',\n",
       "  0],\n",
       " ['1035252480215592966',\n",
       "  '“EVERYbody calling you Nigger now!” https://t.co/6mguJ6KIBF',\n",
       "  0],\n",
       " ['1106978219654303744',\n",
       "  '“ real ass bitch give a fuck boutta nigga” https://t.co/YtyXf52oGi',\n",
       "  0],\n",
       " ['1113920043568463874',\n",
       "  '@WhiteHouse @realDonaldTrump Fuck ice. White supremacist trash. All of you are racist garbage. https://t.co/o2OB4yUnex',\n",
       "  1],\n",
       " ['1114588617693966336', 'Day’s a cunt https://t.co/Ie6QZReHsw', 0],\n",
       " ['1045809514740666370', '#sissy faggot https://t.co/bm1nk8HcYO', 0],\n",
       " ['1108178453910695936', '@Gloriko_ Nigga what? https://t.co/nOwIJtgtU1', 0]]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = lambda labels_list: [0 if x == 0 else 1 for x in labels_list]\n",
    "flattened = [[key, val['tweet_text'], 1 if sum(to_label(val['labels'])) ==3 else 0] \n",
    "             for key, val in hs_data.items()]\n",
    "flattened[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@FriskDontMiss Nigga https://t.co/cAsaLWEpue',\n",
       " 'My horses are retarded https://t.co/HYhqc6d5WN',\n",
       " '“NIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL SHIT NIGGA” https://t.co/UczofqHrLq',\n",
       " 'RT xxSuGVNGxx: I ran into this HOLY NIGGA TODAY 😭😭😭😭 https://t.co/Wa6Spl9kIw',\n",
       " '“EVERYbody calling you Nigger now!” https://t.co/6mguJ6KIBF',\n",
       " '“ real ass bitch give a fuck boutta nigga” https://t.co/YtyXf52oGi',\n",
       " '@WhiteHouse @realDonaldTrump Fuck ice. White supremacist trash. All of you are racist garbage. https://t.co/o2OB4yUnex',\n",
       " 'Day’s a cunt https://t.co/Ie6QZReHsw',\n",
       " '#sissy faggot https://t.co/bm1nk8HcYO',\n",
       " '@Gloriko_ Nigga what? https://t.co/nOwIJtgtU1']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "HD = numpy.array(falttened)\n",
    "HD = falttened\n",
    "HD[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(text_data) -> list:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        analyzer=\"word\",\n",
    "        tokenizer=None,\n",
    "        preprocessor=None,\n",
    "        stop_words=stopwords.words('english'),\n",
    "        max_features=200\n",
    "    )\n",
    "\n",
    "    train_data_features = vectorizer.fit_transform(text_data)\n",
    "    print(f'vocabulary: {vectorizer.vocabulary_}')\n",
    "    print(f'vocabulary size is {len(vectorizer.vocabulary_)}')\n",
    "\n",
    "    return train_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: {'nigga': 116, 'https': 84, 'co': 26, 'retarded': 137, 'real': 132, 'shit': 147, 'rt': 139, 'today': 165, 'calling': 23, 'nigger': 118, 'ass': 4, 'bitch': 14, 'give': 64, 'fuck': 54, 'realdonaldtrump': 133, 'white': 187, 'trash': 168, 'day': 34, 'cunt': 31, 'faggot': 47, 'twitter': 174, 'word': 191, 'fat': 48, 'getting': 63, 'another': 2, 'sjw': 149, 'wanna': 181, 'talk': 156, 'first': 51, 'time': 164, 'says': 144, 'never': 113, 'surrender': 154, 'really': 134, 'said': 141, 'dyke': 42, 'twat': 173, 'always': 0, 'bad': 8, 'still': 151, 'like': 91, 'called': 22, 'man': 105, 'see': 145, 'need': 112, 'little': 93, 'dumb': 41, 'niggas': 117, 'got': 72, 'play': 126, 'look': 97, 'happy': 76, 'big': 12, 'love': 100, 'ya': 195, 'im': 85, 'god': 66, 'damn': 33, 'take': 155, 'card': 24, 'retard': 136, 'ever': 44, 'stop': 152, 'know': 87, 'oh': 121, 'let': 89, 'go': 65, 'get': 61, 'amp': 1, 'new': 114, 'every': 45, 'beat': 9, 'bro': 18, 'fucking': 56, 'make': 104, 'people': 125, 'right': 138, 'want': 182, 'die': 37, 'made': 102, 'good': 71, 'would': 193, 'seen': 146, 'bout': 16, 'lmao': 95, 'da': 32, 'gone': 69, 'yo': 198, 'one': 124, 'free': 53, 'miss': 108, 'going': 67, 'gotta': 73, 'gets': 62, 'whole': 188, 'gonna': 70, 'tell': 158, 'us': 177, 'redneck': 135, 'think': 161, 'hard': 77, 'work': 192, 'hit': 81, 'nobody': 120, 'trying': 171, 'conspiracy': 29, 'theorist': 160, 'best': 10, 'van': 179, 'yes': 197, 'say': 142, 'back': 7, 'head': 79, 'much': 110, 'lil': 92, 'stupid': 153, 'wait': 180, 'guy': 75, 'dick': 36, 'ok': 122, 'young': 199, 'race': 131, 'yeah': 196, 'way': 184, 'better': 11, 'try': 170, 'looking': 98, 'maga': 103, 'buildthewall': 20, 'tf': 159, 'even': 43, 'talking': 157, 'wit': 190, 'funny': 58, 'trailer': 167, 'watch': 183, 'live': 94, 'feel': 49, 'call': 21, 'someone': 150, 'please': 128, 'life': 90, 'ugly': 175, 'gay': 60, 'lol': 96, 'last': 88, 'full': 57, 'babe': 5, 'black': 15, 'looks': 99, 'come': 28, 'hate': 78, 'hillbilly': 80, 'next': 115, 'playing': 127, 'trump': 169, 'keep': 86, 'rube': 140, 'gt': 74, 'milf': 107, 'went': 186, 'game': 59, 'dead': 35, 'dont': 40, 'well': 185, 'night': 119, 'house': 83, 'gon': 68, 'pussy': 129, 'mad': 101, 'win': 189, 'put': 130, 'face': 46, 'wtf': 194, 'tho': 162, 'thought': 163, 'cock': 27, 'fuckin': 55, 'follow': 52, 'birthday': 13, 'ur': 176, 'bruh': 19, 'dis': 38, 'show': 148, 'find': 50, 'mean': 106, 'tryna': 172, 'told': 166, 'cause': 25, 'money': 109, 'around': 3, 'name': 111, 'old': 123, 'baby': 6, 'done': 39, 'could': 30, 'hot': 82, 'saying': 143, 'use': 178, 'boy': 17}\n",
      "vocabulary size is 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(149823, 200)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized = vectorizer(HD)\n",
    "vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = vectorized\n",
    "y = [row[2] for row in HD]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='sgd',\n",
    "                    alpha=1e-5,\n",
    "                    hidden_layer_sizes=(100,),\n",
    "                    random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "results = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is: 0.1653\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, results)\n",
    "print(\"Accuracy score is: %.4f\\n\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in c:\\programdata\\anaconda3\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: Cython==0.29.21 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (0.29.21)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (5.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsVec = [nltk.word_tokenize(tweet_text) for tweet_text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nigga'],\n",
       " ['hors', 'retard'],\n",
       " ['nigga', 'momma', 'youngboy', 'spit', 'real', 'shit', 'nigga'],\n",
       " ['xxsugvngxx', 'ran', 'holi', 'nigga', 'today'],\n",
       " ['everybodi', 'call', 'nigger'],\n",
       " ['real', 'ass', 'bitch', 'give', 'fuck', 'boutta', 'nigga'],\n",
       " ['fuck', 'ice', 'white', 'supremacist', 'trash', 'racist', 'garbag'],\n",
       " ['day', 'cunt'],\n",
       " ['sissi', 'faggot'],\n",
       " ['nigga']]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsVec[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "vec_model =gensim.models.Word2Vec(\n",
    "    hsVec, \n",
    "    vector_size=100, \n",
    "    window=5, \n",
    "    min_count=1, \n",
    "    workers=4)\n",
    "vec_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hoe', 0.8540602326393127),\n",
       " ('dat', 0.8509646654129028),\n",
       " ('wit', 0.8476145267486572),\n",
       " ('cuz', 0.8444004058837891),\n",
       " ('imma', 0.8442403674125671)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 5 similar word to \"nigga\"\n",
    "word=\"nigga\"\n",
    "vec_model.wv.most_similar (positive=word,topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ignor', 0.9185754656791687),\n",
       " ('poor', 0.9153494834899902),\n",
       " ('support', 0.9071479439735413),\n",
       " ('nazi', 0.9054499268531799),\n",
       " ('inbr', 0.8917744755744934)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 5 similar word to \"racist\"\n",
    "word2=\"racist\"\n",
    "vec_model.wv.most_similar (positive=word2,topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lil', 0.9541468620300293),\n",
       " ('bitch', 0.952037513256073),\n",
       " ('mf', 0.9506526589393616),\n",
       " ('bruh', 0.9500959515571594),\n",
       " ('dawg', 0.948303759098053),\n",
       " ('fuckin', 0.9452494382858276),\n",
       " ('shawty', 0.9435693025588989),\n",
       " ('ass', 0.9413026571273804),\n",
       " ('cuz', 0.9359691739082336),\n",
       " ('shit', 0.9340022802352905)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
    "glove_vectors.most_similar('nigga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
